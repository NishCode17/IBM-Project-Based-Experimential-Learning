{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XslpQb3MAxdu",
    "outputId": "34cbddbf-2c6a-4cfb-c25a-888642ef6808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wtlRy7hwDSRu"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCM9OkQDEbYE",
    "outputId": "c20ea3a3-51ed-4d3b-ee8c-15788cb18680"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pipeline\n",
    "clf = pipeline(\"text-classification\", model=\"jy46604790/Fake-News-Bert-Detect\", tokenizer=\"jy46604790/Fake-News-Bert-Detect\")\n",
    "\n",
    "def detect_fake_news(text):\n",
    "    result = clf(text[:500])[0]  # truncates after 500 chars\n",
    "    label = \"FAKE\" if result['label']==\"LABEL_0\" else \"REAL\"\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    print(f\"Prediction: {label} (Confidence: {result['score']:.2f})\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSdP4clSEu7z",
    "outputId": "df2b430c-2f42-4280-e439-f3e8d244d93d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“° Fake News Generator & Detector\n",
      "\n",
      "\n",
      "Choose an option:\n",
      "1 - Generate Fake News Headlines using GPT-2\n",
      "2 - Detect Fake/Real Headline using BERT\n",
      "Type 'exit' to quit.\n",
      "Enter 1 / 2 / exit: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§  Generated Fake Headlines:\n",
      "\n",
      "1. Breaking News: What do you think about the GOP's failure to repeal the Affordable Care Act, and do you think that it will succeed?\n",
      "\n",
      "\n",
      "2. Breaking News: I think the biggest challenge for us is how do we continue to put it together as a family and our kids can continue to play hockey\n",
      "3. Breaking News: Who can you give your email address?\n",
      "\n",
      "Mueller said he wanted to \"do a little bit of homework,\" and that he\n",
      "4. Breaking News: 'All About Benghazi'\n",
      "\n",
      "The White House condemned the attack, while the Democratic National Committee said it was responding to the outrage.\n",
      "5. Breaking News: We were working on this in the office where I work, and a friend came over to say, \"Hey, this guy just had\n",
      "\n",
      "Choose an option:\n",
      "1 - Generate Fake News Headlines using GPT-2\n",
      "2 - Detect Fake/Real Headline using BERT\n",
      "Type 'exit' to quit.\n",
      "Enter 1 / 2 / exit: 2\n",
      "\n",
      "ğŸ” Enter a news headline (type 'exit' to go back):\n",
      "ğŸ“ Headline: America declares war on China, japan nuked\n",
      "\n",
      "Input: America declares war on China, japan nuked\n",
      "Prediction: FAKE (Confidence: 0.99)\n",
      "\n",
      "ğŸ“ Headline: Argentina won the world cup\n",
      "\n",
      "Input: Argentina won the world cup\n",
      "Prediction: REAL (Confidence: 0.82)\n",
      "\n",
      "ğŸ“ Headline: Modi wins 3rd term\n",
      "\n",
      "Input: Modi wins 3rd term\n",
      "Prediction: REAL (Confidence: 0.99)\n",
      "\n",
      "ğŸ“ Headline: Trump wins his 2nd term beating Biden\n",
      "\n",
      "Input: Trump wins his 2nd term beating Biden\n",
      "Prediction: FAKE (Confidence: 0.60)\n",
      "\n",
      "ğŸ“ Headline: exit\n",
      "\n",
      "Choose an option:\n",
      "1 - Generate Fake News Headlines using GPT-2\n",
      "2 - Detect Fake/Real Headline using BERT\n",
      "Type 'exit' to quit.\n",
      "Enter 1 / 2 / exit: exit\n",
      "ğŸ‘‹ Exiting the system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“° Fake News Generator & Detector\\n\")\n",
    "\n",
    "while True:\n",
    "    print(\"\\nChoose an option:\")\n",
    "    print(\"1 - Generate Fake News Headlines using GPT-2\")\n",
    "    print(\"2 - Detect Fake/Real Headline using BERT\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "\n",
    "    choice = input(\"Enter 1 / 2 / exit: \").strip().lower()\n",
    "\n",
    "    if choice == '1':\n",
    "        prompt = \"Breaking News:\"\n",
    "        input_ids = gpt2_tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "        outputs = gpt2_model.generate(\n",
    "            input_ids,\n",
    "            max_length=30,\n",
    "            num_return_sequences=5,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "\n",
    "        print(\"\\nğŸ§  Generated Fake Headlines:\\n\")\n",
    "        for i, output in enumerate(outputs):\n",
    "            headline = gpt2_tokenizer.decode(output, skip_special_tokens=True)\n",
    "            print(f\"{i+1}. {headline}\")\n",
    "\n",
    "    elif choice == '2':\n",
    "        print(\"\\nğŸ” Enter a news headline (type 'exit' to go back):\")\n",
    "        while True:\n",
    "            text = input(\"ğŸ“ Headline: \").strip()\n",
    "            if text.lower() == 'exit':\n",
    "                break\n",
    "            detect_fake_news(text)\n",
    "\n",
    "    elif choice == 'exit':\n",
    "        print(\"ğŸ‘‹ Exiting the system. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ Invalid input. Please enter 1, 2, or 'exit'.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
